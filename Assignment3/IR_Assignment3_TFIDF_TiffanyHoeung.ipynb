{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tXEQdv-tKYDp"
      },
      "outputs": [],
      "source": [
        "from post_parser_record import PostParserRecord\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import math\n",
        "from itertools import islice\n",
        "import csv, os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus.reader.tagged import word_tokenize\n",
        "import re, string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "rLTOKHbdMjQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5db5c83-ba99-462f-f13e-7febe26f4c0c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createDocIDF():\n",
        "  total_docs = len(post_reader.map_questions) + len(post_reader.map_just_answers)\n",
        "  for word in term_total_docs:\n",
        "    doc_idf[word] = math.log(total_docs / float(term_total_docs[word]))"
      ],
      "metadata": {
        "id": "rCAZJBt1jUQK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createDocTF():\n",
        "  for answer_id in doc_term_count:\n",
        "    words = doc_term_count[answer_id]\n",
        "    doc_dic = {}\n",
        "    for word in words:\n",
        "      doc_dic[word] = words[word] / doc_total_terms[answer_id]\n",
        "    doc_tf[answer_id] = doc_dic.copy()"
      ],
      "metadata": {
        "id": "qG8MdRgljRzh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_term_count = {}     # {doc: {term:count, term:count}, doc:{term:count,...},...}\n",
        "doc_total_terms = {}    # {doc: #_of_terms, doc: #_of_terms, ...}\n",
        "term_total_docs = {}    # {term:count, term:count, ...}.\n",
        "\n",
        "def createIndex():\n",
        "  # For each question\n",
        "  for answer_id in post_reader.map_questions:\n",
        "    # Get the text\n",
        "    text = (post_reader.map_questions[answer_id].title + \" \" + post_reader.map_questions[answer_id].body)\n",
        "\n",
        "    # Remove punctuations, make lowercase\n",
        "    token_words = re.sub(\"<.*?>|\\\\n|&quot;\", \" \", text.lower())\n",
        "    token_words = word_tokenize(token_words.translate(str.maketrans('', '', string.punctuation)))\n",
        "    token_words = [word for word in token_words if word not in stop_words]\n",
        "\n",
        "    # Save a dictionary with {term:count, term:count, term:count}\n",
        "    doc_dic = {}\n",
        "    count = 0;\n",
        "    for word in token_words:\n",
        "      if word not in stop_words:\n",
        "        count += 1;\n",
        "        if word in doc_dic:\n",
        "          doc_dic[word] += 1\n",
        "        else:\n",
        "          doc_dic[word] = 1;\n",
        "\n",
        "        if word not in term_total_docs:\n",
        "          term_total_docs[word] = 1\n",
        "        else:\n",
        "          term_total_docs[word] += 1;\n",
        "    # Save dictionary to doc_term_count as {docid: {term:count, term:count}, ...}\n",
        "    doc_total_terms[answer_id] = count;\n",
        "    doc_term_count[answer_id] = doc_dic.copy()\n",
        "\n",
        "  # For each question\n",
        "  for answer_id in post_reader.map_just_answers:\n",
        "    # Get the text\n",
        "    text = (post_reader.map_just_answers[answer_id].body)\n",
        "\n",
        "    # Remove punctuations, make lowercase\n",
        "    token_words = re.sub(\"<.*?>|\\\\n|&quot;\", \" \", text.lower())\n",
        "    token_words = word_tokenize(token_words.translate(str.maketrans('', '', string.punctuation)))\n",
        "    token_words = [word for word in token_words if word not in stop_words]\n",
        "\n",
        "    # Save a dictionary with {term:count, term:count, term:count}\n",
        "    doc_dic = {}\n",
        "    count = 0;\n",
        "    for word in token_words:\n",
        "      if word not in stop_words:\n",
        "        count += 1;\n",
        "        if word in doc_dic:\n",
        "          doc_dic[word] += 1\n",
        "        else:\n",
        "          doc_dic[word] = 1;\n",
        "\n",
        "        if word not in term_total_docs:\n",
        "          term_total_docs[word] = 1\n",
        "        else:\n",
        "          term_total_docs[word] += 1;\n",
        "    doc_total_terms[answer_id] = count;\n",
        "    # Save dictionary to doc_term_count as {docid: {term:count, term:count}, ...}\n",
        "    doc_term_count[answer_id] = doc_dic.copy()\n",
        "\n",
        "  createDocTF()\n",
        "  createDocIDF()"
      ],
      "metadata": {
        "id": "9Nm_WuSqK1Wm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TF Dictionary\n",
        "doc_tf = {}\n",
        "\n",
        "# Create the IDF Dictionary\n",
        "doc_idf = {}\n",
        "\n",
        "# Index through posts\n",
        "createIndex()"
      ],
      "metadata": {
        "id": "jXH_HkjPapKx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTFIDFScore(query):\n",
        "  query_parse = query.split(\" \")\n",
        "  word_scores = {}     # {word: {docid: tf-idf, docid: tf-idf}, word: {docid: tf-idf}}\n",
        "  for word in query_parse:\n",
        "    if word in doc_idf:\n",
        "      idf = doc_idf[word] # Get our current term IDF\n",
        "      # For every document:\n",
        "      for doc_id in doc_tf:\n",
        "        # Check if the term exists in the document:\n",
        "        if word in doc_tf[doc_id]:\n",
        "          if word in word_scores:\n",
        "            curr_score = word_scores[word]\n",
        "            curr_score[doc_id] = idf * doc_tf[doc_id][word]\n",
        "          else:\n",
        "            word_scores[word] = {doc_id: (idf * doc_tf[doc_id][word])}\n",
        "        else:\n",
        "          if word in word_scores:\n",
        "            curr_score = word_scores[word]\n",
        "            curr_score[doc_id] = 0\n",
        "          else:\n",
        "            word_scores[word] = {doc_id: 0}\n",
        "  # at this point, we need to sum all like doc_ids to get final scores\n",
        "  doc_scores = {} # {docid: score, docid: score, ...}\n",
        "  for word_docs in word_scores: \n",
        "    word_tfidfs = word_scores[word_docs] #word_tfidfs = {docid: tf-idf, docid: tf-idf...}\n",
        "    for doc_id in word_tfidfs:\n",
        "      score = word_tfidfs[doc_id]\n",
        "      if doc_id in doc_scores:\n",
        "        doc_scores[doc_id] = doc_scores[doc_id] + score\n",
        "      else:\n",
        "        doc_scores[doc_id] = score\n",
        "\n",
        "  return doc_scores"
      ],
      "metadata": {
        "id": "4a_0RC4WWVgC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printScores(query, scores):\n",
        "  print(query + \": \")\n",
        "  for doc in scores:\n",
        "    print('%d \\t\\t %.2f' % (doc, scores[doc]))\n",
        "  print()"
      ],
      "metadata": {
        "id": "1l9nVwIOBTWb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_results = []\n",
        "# Now that we have TF and IDF, we need our query:\n",
        "query = \"espresso\"\n",
        "scores = getTFIDFScore(query)\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "query_results.append(scores)\n",
        "\n",
        "query = \"turkish coffee\"\n",
        "scores = getTFIDFScore(query)\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "query_results.append(scores)\n",
        "\n",
        "query = \"making a decaffeinated coffee\"\n",
        "scores = getTFIDFScore(query)\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "query_results.append(scores)\n",
        "\n",
        "query = \"can i use the same coffee grounds twice\"\n",
        "scores = getTFIDFScore(query)\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "query_results.append(scores)\n",
        "\n",
        "print(query_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXBXmHBeVz4G",
        "outputId": "664e9882-eb3f-450a-e6ee-5a2510f3daca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "espresso: \n",
            "4404 \t\t 0.14\n",
            "2867 \t\t 0.13\n",
            "3168 \t\t 0.12\n",
            "3904 \t\t 0.12\n",
            "3800 \t\t 0.11\n",
            "\n",
            "turkish coffee: \n",
            "5182 \t\t 0.38\n",
            "483 \t\t 0.29\n",
            "5094 \t\t 0.28\n",
            "209 \t\t 0.27\n",
            "4486 \t\t 0.25\n",
            "\n",
            "making a decaffeinated coffee: \n",
            "204 \t\t 0.55\n",
            "120 \t\t 0.52\n",
            "2897 \t\t 0.31\n",
            "3225 \t\t 0.27\n",
            "3293 \t\t 0.22\n",
            "\n",
            "can i use the same coffee grounds twice: \n",
            "2683 \t\t 0.35\n",
            "3966 \t\t 0.23\n",
            "5582 \t\t 0.19\n",
            "3818 \t\t 0.18\n",
            "2585 \t\t 0.17\n",
            "\n",
            "[{4404: 0.14, 2867: 0.13, 3168: 0.12, 3904: 0.12, 3800: 0.11}, {5182: 0.38, 483: 0.29, 5094: 0.28, 209: 0.27, 4486: 0.25}, {204: 0.55, 120: 0.52, 2897: 0.31, 3225: 0.27, 3293: 0.22}, {2683: 0.35, 3966: 0.23, 5582: 0.19, 3818: 0.18, 2585: 0.17}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevance_docs = {{doc: 0, doc: 1, doc: 0, ...}, {doc: 0, doc: 1, ...}}\n",
        "# Results = {{doc: score, doc: score, doc: score}, {doc: score, doc:score, ...}}\n",
        "def createQrel(relevance_docs, results, filename):\n",
        "  qrel_filename = os.path.join(\"/content\", (filename + \".txt\"))\n",
        "  qrelresults_filename = os.path.join(\"/content\", (filename + \"Results.txt\"))\n",
        "  question = \"Q00\"\n",
        "  count = 1;\n",
        "\n",
        "  with open(qrel_filename, 'w') as f:\n",
        "    for relevance in relevance_docs:\n",
        "      question_str = question + str(count)\n",
        "      count += 1\n",
        "      for doc in relevance:\n",
        "        line = question_str + \" 0 \" + str(doc) + \" \" + str(relevance[doc])\n",
        "        f.write(line + \"\\n\")\n",
        "  \n",
        "  count = 1\n",
        "  with open(qrelresults_filename, 'w') as f:\n",
        "    for query_results in results:\n",
        "      question_str = question + str(count)\n",
        "      count += 1\n",
        "      for doc in query_results:\n",
        "        line = question_str + \" Q0 \" + str(doc) + \" 1 \" + str(query_results[doc]) + \" TF-IDF\"\n",
        "        f.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "BJ6iLiONvk7W"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevance \n",
        "doc_relevance = [{4404: 0, 2867: 0, 3168: 0, 3904: 0, 3800: 0}, \n",
        "                 {5182: 1, 483: 0, 5094: 0, 209: 1, 4486: 0}, \n",
        "                 {204: 0, 120: 0, 2897: 0, 3225: 1, 3293: 0}, \n",
        "                 {2683: 1, 3966: 0, 5582: 1, 3818: 0, 2585: 0}]\n",
        "\n",
        "createQrel(doc_relevance, query_results, \"CoffeePostsTFIDF\")"
      ],
      "metadata": {
        "id": "m0KMnUo-vgYR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### UNCOMMENT TO VIEW VALUES:\n",
        "# Keeps track of {docid: {term:count, term:count,...}, docid: {term:count, term:count}, ...}\n",
        "#doc_term_count\n",
        "\n",
        "# Keeps track of {docid: total_term_count, docid: total_term_count, ...}\n",
        "#doc_total_terms\n",
        "\n",
        "# Keeps track of {term: #_of_docs, term: #_of_docs, ...}\n",
        "#term_total_docs\n",
        "\n",
        "# Keeps track of {docid: {term:TERM_FREQ, term:TERM_FREQ}, docid: {term:TERM_FREQ}, ...}\n",
        "#doc_tf\n",
        "\n",
        "# Keeps track of {term: idf_score, term: idf_score}\n",
        "#doc_idf"
      ],
      "metadata": {
        "id": "mYjfXpz9b3mA"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}