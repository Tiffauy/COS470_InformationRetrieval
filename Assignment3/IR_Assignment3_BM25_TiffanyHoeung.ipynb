{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EjpkQS9GBhkZ"
      },
      "outputs": [],
      "source": [
        "from post_parser_record import PostParserRecord\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import math\n",
        "from itertools import islice\n",
        "import nltk\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus.reader.tagged import word_tokenize\n",
        "import re, string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Qg8piABlEX",
        "outputId": "f5979265-1679-4080-9d24-1e17345ebd71"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createDocIDF():\n",
        "  total_docs = len(post_reader.map_questions) + len(post_reader.map_just_answers)\n",
        "  for word in term_total_docs:\n",
        "    doc_idf[word] = math.log(total_docs / float(term_total_docs[word]))"
      ],
      "metadata": {
        "id": "wRL1hNnCBwC3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createDocTF():\n",
        "  for answer_id in doc_term_count:\n",
        "    words = doc_term_count[answer_id]\n",
        "    doc_dic = {}\n",
        "    for word in words:\n",
        "      doc_dic[word] = words[word] / doc_total_terms[answer_id]\n",
        "    doc_tf[answer_id] = doc_dic.copy()"
      ],
      "metadata": {
        "id": "LBDUFUhVBvN-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_total_docs = {}    # {term:count, term:count, ...}.\n",
        "\n",
        "def createIndex():\n",
        "  # For each question\n",
        "  for answer_id in post_reader.map_questions:\n",
        "    # Get the text\n",
        "    text = (post_reader.map_questions[answer_id].title + \" \" + post_reader.map_questions[answer_id].body)\n",
        "\n",
        "    # Remove punctuations, make lowercase\n",
        "    token_words = re.sub(\"<.*?>|\\\\n|&quot;\", \" \", text.lower())\n",
        "    token_words = word_tokenize(token_words.translate(str.maketrans('', '', string.punctuation)))\n",
        "    token_words = [word for word in token_words if word not in stop_words]\n",
        "\n",
        "    # Save a dictionary with {term:count, term:count, term:count}\n",
        "    doc_dic = {}\n",
        "    count = 0;\n",
        "    for word in token_words:\n",
        "      count += 1;\n",
        "      if word in doc_dic:\n",
        "        doc_dic[word] += 1\n",
        "      else:\n",
        "        doc_dic[word] = 1;\n",
        "      if word not in term_total_docs:\n",
        "        term_total_docs[word] = 1\n",
        "      else:\n",
        "        term_total_docs[word] += 1;\n",
        "    # Save dictionary to doc_term_count as {docid: {term:count, term:count}, ...}\n",
        "    doc_total_terms[answer_id] = count;\n",
        "    doc_term_count[answer_id] = doc_dic.copy()\n",
        "\n",
        "  # For each question\n",
        "  for answer_id in post_reader.map_just_answers:\n",
        "    # Get the text\n",
        "    text = (post_reader.map_just_answers[answer_id].body)\n",
        "\n",
        "    # Remove punctuations, make lowercase\n",
        "    token_words = re.sub(\"<.*?>|\\\\n|&quot;\", \" \", text.lower())\n",
        "    token_words = word_tokenize(token_words.translate(str.maketrans('', '', string.punctuation)))\n",
        "    token_words = [word for word in token_words if word not in stop_words]\n",
        "\n",
        "    # Save a dictionary with {term:count, term:count, term:count}\n",
        "    doc_dic = {}\n",
        "    count = 0;\n",
        "    for word in token_words:\n",
        "      count += 1;\n",
        "      if word in doc_dic:\n",
        "        doc_dic[word] += 1\n",
        "      else:\n",
        "        doc_dic[word] = 1;\n",
        "\n",
        "      if word not in term_total_docs:\n",
        "        term_total_docs[word] = 1\n",
        "      else:\n",
        "        term_total_docs[word] += 1;\n",
        "    doc_total_terms[answer_id] = count;\n",
        "    # Save dictionary to doc_term_count as {docid: {term:count, term:count}, ...}\n",
        "    doc_term_count[answer_id] = doc_dic.copy()\n",
        "\n",
        "  createDocTF()\n",
        "  createDocIDF()"
      ],
      "metadata": {
        "id": "7BF28A0KBqIj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAverageDocLength(documents):\n",
        "  size = len(documents)\n",
        "  count = 0\n",
        "  for doc in documents:\n",
        "    count += documents[doc]\n",
        "  return count / size"
      ],
      "metadata": {
        "id": "WdiV9zMsEk6f"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getBM25Score(query):\n",
        "  scores = {}\n",
        "  # Equation: IDF * (A / B)\n",
        "  # A: (1.2 + 1) * tf\n",
        "  # B: 1.2 * ((1 - 0.75) + 0.75 * (doc length / avg doc length)) + tf\n",
        "  k1 = 1.2\n",
        "  b = 0.75\n",
        "  query_parse = query.split(\" \")\n",
        "  for word in query_parse:\n",
        "    for doc_id in doc_tf:\n",
        "      try:\n",
        "        tf = doc_tf[doc_id][word]\n",
        "      except:\n",
        "        tf = 0\n",
        "      try:\n",
        "        idf = doc_idf[word]\n",
        "      except:\n",
        "        idf = 0\n",
        "      doc_length = doc_total_terms[doc_id]\n",
        "      A = (k1 + 1) * tf\n",
        "      B = k1 * ((1 - b) + b * (doc_length/avg_doc_length)) + tf\n",
        "      if doc_id in scores:\n",
        "        scores[doc_id] = scores[doc_id] + (idf * (A / B))\n",
        "      else:\n",
        "        scores[doc_id] = (idf * (A/B))\n",
        "  \n",
        "  return scores"
      ],
      "metadata": {
        "id": "rL_gLS3UDnry"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printScores(query, scores):\n",
        "  print(query + \": \")\n",
        "  for doc in scores:\n",
        "    print('%d \\t\\t %.2f' % (doc, scores[doc]))\n",
        "  print()"
      ],
      "metadata": {
        "id": "zTx2aMahDnR-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TF Dictionary\n",
        "doc_tf = {}             # {docid: {term:TERM_FREQ, term:TERM_FREQ}, docid: {term:TERM_FREQ}, ...}\n",
        "\n",
        "# Create the IDF Dictionary\n",
        "doc_idf = {}            # {term: idf_score, term: idf_score}\n",
        "\n",
        "# Keeps track of term counts for each document\n",
        "doc_term_count = {}     # {doc: {term:count, term:count}, doc:{term:count,...},...}\n",
        "\n",
        "# Keeps track of total number of terms for each document\n",
        "doc_total_terms = {}    # {doc: #_of_terms, doc: #_of_terms, ...}\n",
        "\n",
        "# Index through posts\n",
        "createIndex()\n",
        "\n",
        "# Get average document length for BM25 calculations\n",
        "avg_doc_length = getAverageDocLength(doc_total_terms)"
      ],
      "metadata": {
        "id": "5rjCzspHBwt2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_results = []\n",
        "# Now that we have TF and IDF, we need our query:\n",
        "query = \"espresso\"\n",
        "scores = getBM25Score(query)\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "query_results.append(scores)\n",
        "\n",
        "query = \"turkish coffee\"\n",
        "scores = getBM25Score(query)\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "query_results.append(scores)\n",
        "\n",
        "query = \"making a decaffeinated coffee\"\n",
        "scores = getBM25Score(query)\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "query_results.append(scores)\n",
        "\n",
        "query = \"can i use the same coffee grounds twice\"\n",
        "scores = getBM25Score(query)\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "query_results.append(scores)"
      ],
      "metadata": {
        "id": "khnHh2UFBzaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3095f21c-1e8d-40b9-84d1-bec0bc9e012c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "espresso: \n",
            "4404 \t\t 0.51\n",
            "3904 \t\t 0.49\n",
            "2867 \t\t 0.43\n",
            "3168 \t\t 0.38\n",
            "4619 \t\t 0.34\n",
            "\n",
            "turkish coffee: \n",
            "5182 \t\t 1.50\n",
            "5094 \t\t 1.13\n",
            "483 \t\t 1.05\n",
            "4486 \t\t 1.03\n",
            "209 \t\t 0.98\n",
            "\n",
            "making a decaffeinated coffee: \n",
            "204 \t\t 1.96\n",
            "120 \t\t 1.65\n",
            "3293 \t\t 1.08\n",
            "3225 \t\t 1.07\n",
            "2897 \t\t 1.05\n",
            "\n",
            "can i use the same coffee grounds twice: \n",
            "2683 \t\t 0.97\n",
            "3966 \t\t 0.87\n",
            "3818 \t\t 0.69\n",
            "5582 \t\t 0.66\n",
            "4703 \t\t 0.60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### UNCOMMENT TO VIEW VALUES:\n",
        "# Keeps track of {docid: {term:count, term:count,...}, docid: {term:count, term:count}, ...}\n",
        "#doc_term_count\n",
        "\n",
        "# Keeps track of {docid: total_term_count, docid: total_term_count, ...}\n",
        "#doc_total_terms\n",
        "\n",
        "# Keeps track of {term: #_of_docs, term: #_of_docs, ...}\n",
        "#term_total_docs\n",
        "\n",
        "# Keeps track of {docid: {term:TERM_FREQ, term:TERM_FREQ}, docid: {term:TERM_FREQ}, ...}\n",
        "#doc_tf\n",
        "\n",
        "# Keeps track of {term: idf_score, term: idf_score}\n",
        "#doc_idf"
      ],
      "metadata": {
        "id": "o6ytxMDnByMk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevance_docs = {{doc: 0, doc: 1, doc: 0, ...}, {doc: 0, doc: 1, ...}}\n",
        "# Results = {{doc: score, doc: score, doc: score}, {doc: score, doc:score, ...}}\n",
        "def createQrel(relevance_docs, results, filename):\n",
        "  qrel_filename = os.path.join(\"/content\", (filename + \".txt\"))\n",
        "  qrelresults_filename = os.path.join(\"/content\", (filename + \"Results.txt\"))\n",
        "  question = \"Q00\"\n",
        "  count = 1;\n",
        "\n",
        "  with open(qrel_filename, 'w') as f:\n",
        "    for relevance in relevance_docs:\n",
        "      question_str = question + str(count)\n",
        "      count += 1\n",
        "      for doc in relevance:\n",
        "        line = question_str + \" 0 \" + str(doc) + \" \" + str(relevance[doc])\n",
        "        f.write(line + \"\\n\")\n",
        "  \n",
        "  count = 1\n",
        "  with open(qrelresults_filename, 'w') as f:\n",
        "    for query_results in results:\n",
        "      question_str = question + str(count)\n",
        "      count += 1\n",
        "      for doc in query_results:\n",
        "        line = question_str + \" Q0 \" + str(doc) + \" 1 \" + str(query_results[doc]) + \" BM25\"\n",
        "        f.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "Sz4aobn31Tk4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevance \n",
        "doc_relevance = [{4404: 0, 3904: 0, 2867: 0, 3168: 0, 4619: 0}, \n",
        "                 {5182: 1, 5094: 0, 483: 0, 4486: 0, 209: 1}, \n",
        "                 {204: 0, 120: 0, 3293: 0, 3225: 1, 2897: 0}, \n",
        "                 {2683: 1, 3966: 0, 3818: 0, 5582: 1, 4703: 0}]\n",
        "\n",
        "createQrel(doc_relevance, query_results, \"CoffeePostsBM25\")"
      ],
      "metadata": {
        "id": "XJQsoGQz1Qz9"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}