{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JN_bcy177MFY"
      },
      "outputs": [],
      "source": [
        "from post_parser_record import PostParserRecord\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import math\n",
        "from scipy import spatial\n",
        "import os\n",
        "from itertools import islice\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus.reader.tagged import word_tokenize\n",
        "import re, string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX3_WQgP7Rpw",
        "outputId": "16c3e702-e622-4a19-c6f1-98166d8c4662"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getPostTerms():\n",
        "  terms = []\n",
        "  # For each question\n",
        "  for answer_id in post_reader.map_questions:\n",
        "    # Get the text\n",
        "    text = (post_reader.map_questions[answer_id].title + \" \" + post_reader.map_questions[answer_id].body)\n",
        "\n",
        "    # Remove punctuations, make lowercase\n",
        "    token_words = re.sub(\"<.*?>|\\\\n|&quot;\", \" \", text.lower())\n",
        "    token_words = word_tokenize(token_words.translate(str.maketrans('', '', string.punctuation)))\n",
        "    token_words = [word for word in token_words if word not in stop_words]\n",
        "\n",
        "    # Save terms\n",
        "    for word in token_words:\n",
        "      terms.append(word)\n",
        "\n",
        "  # For each question\n",
        "  for answer_id in post_reader.map_just_answers:\n",
        "    # Get the text\n",
        "    text = (post_reader.map_just_answers[answer_id].body)\n",
        "\n",
        "    # Remove punctuations, make lowercase\n",
        "    token_words = re.sub(\"<.*?>|\\\\n|&quot;\", \" \", text.lower())\n",
        "    token_words = word_tokenize(token_words.translate(str.maketrans('', '', string.punctuation)))\n",
        "    token_words = [word for word in token_words if word not in stop_words]\n",
        "\n",
        "    # Save terms\n",
        "    for word in token_words:\n",
        "      terms.append(word)\n",
        "\n",
        "  return terms"
      ],
      "metadata": {
        "id": "1YiIJBiLkYao"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateTermIndices(terms):\n",
        "  temp_dic = {}\n",
        "  count = 0\n",
        "  for term in terms:\n",
        "    temp_dic[term] = count\n",
        "    count += 1\n",
        "  return temp_dic.copy()"
      ],
      "metadata": {
        "id": "y22DK6ucm6rV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateDocVectors(indices, size):\n",
        "  doc_to_vector = {}\n",
        "  for doc_id in post_reader.map_questions:\n",
        "    # Get the text\n",
        "    question = (post_reader.map_questions[doc_id].title + \" \" + post_reader.map_questions[doc_id].body)\n",
        "    doc_to_vector[doc_id] = generateSingleVector(indices, size, question).copy()\n",
        "    \n",
        "  for doc_id in post_reader.map_just_answers:\n",
        "    answer = post_reader.map_just_answers[doc_id].body\n",
        "    doc_to_vector[doc_id] = generateSingleVector(indices, size, answer).copy()\n",
        "\n",
        "  return doc_to_vector"
      ],
      "metadata": {
        "id": "PgURUrsxnzZg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateSingleVector(indices, size, text):\n",
        "  current_vector = [0] * size\n",
        "\n",
        "  # Remove punctuations, make lowercase\n",
        "  token_words = re.sub(\"<.*?>|\\\\n|&quot;\", \" \", text.lower())\n",
        "  token_words = word_tokenize(token_words.translate(str.maketrans('', '', string.punctuation)))\n",
        "  token_words = [word for word in token_words if word not in stop_words]\n",
        "\n",
        "  for word in token_words:\n",
        "    if word in indices:\n",
        "      current_vector[indices[word]] += 1;\n",
        "  \n",
        "  return current_vector"
      ],
      "metadata": {
        "id": "3p8h-79uoZLi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getVSMScores(query, vectors, indices, size):\n",
        "  doc_scores = {}\n",
        "  # Create vector for query\n",
        "  query_vector = generateSingleVector(indices, size, query)\n",
        "\n",
        "  # For every document, get cosine similarity between query vector and doc vector\n",
        "  for doc_id in vectors:\n",
        "    doc_vector = vectors[doc_id]\n",
        "    # Find cosine similarity:\n",
        "    result = 1 - spatial.distance.cosine(query_vector, doc_vector)\n",
        "    doc_scores[doc_id] = result\n",
        "  return doc_scores"
      ],
      "metadata": {
        "id": "daTkrem-r56V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printScores(query, scores):\n",
        "  print(query + \": \")\n",
        "  for doc in scores:\n",
        "    print('%d \\t\\t %.2f' % (doc, scores[doc]))\n",
        "  print()"
      ],
      "metadata": {
        "id": "RJtJ0aMYtyL7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-unique terms:\n",
        "all_terms = getPostTerms()\n",
        "\n",
        "# Unique terms:\n",
        "unique_terms = set(all_terms)\n",
        "\n",
        "# Create dic of {term: index}\n",
        "term_index = generateTermIndices(unique_terms)\n",
        "\n",
        "# Create vectors for every word\n",
        "doc_vectors = generateDocVectors(term_index, len(unique_terms))"
      ],
      "metadata": {
        "id": "du0GpP0BlwWG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_results = []\n",
        "# Queries:\n",
        "query = \"espresso\"\n",
        "scores = getVSMScores(query, doc_vectors, term_index, len(unique_terms))\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "\n",
        "query = \"turkish coffee\"\n",
        "scores = getVSMScores(query, doc_vectors, term_index, len(unique_terms))\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "\n",
        "query = \"making a decaffeinated coffee\"\n",
        "scores = getVSMScores(query, doc_vectors, term_index, len(unique_terms))\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)\n",
        "\n",
        "query = \"can i use the same coffee grounds twice\"\n",
        "scores = getVSMScores(query, doc_vectors, term_index, len(unique_terms))\n",
        "scores = (dict(islice(sorted(scores.items(), key=lambda item: item[1], reverse=True), 5)))\n",
        "for doc in scores:\n",
        "  scores[doc] = round(scores[doc], 2)\n",
        "printScores(query, scores)"
      ],
      "metadata": {
        "id": "Y2JA5JphroWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevance_docs = {{doc: 0, doc: 1, doc: 0, ...}, {doc: 0, doc: 1, ...}}\n",
        "# Results = {{doc: score, doc: score, doc: score}, {doc: score, doc:score, ...}}\n",
        "def createQrel(relevance_docs, results, filename):\n",
        "  qrel_filename = os.path.join(\"/content\", (filename + \".txt\"))\n",
        "  qrelresults_filename = os.path.join(\"/content\", (filename + \"Results.txt\"))\n",
        "  question = \"Q00\"\n",
        "  count = 1;\n",
        "\n",
        "  with open(qrel_filename, 'w') as f:\n",
        "    for relevance in relevance_docs:\n",
        "      question_str = question + str(count)\n",
        "      count += 1\n",
        "      for doc in relevance:\n",
        "        line = question_str + \" 0 \" + str(doc) + \" \" + str(relevance[doc])\n",
        "        f.write(line + \"\\n\")\n",
        "  \n",
        "  count = 1\n",
        "  with open(qrelresults_filename, 'w') as f:\n",
        "    for query_results in results:\n",
        "      question_str = question + str(count)\n",
        "      count += 1\n",
        "      for doc in query_results:\n",
        "        line = question_str + \" Q0 \" + str(doc) + \" 1 \" + str(query_results[doc]) + \" TF-IDF\"\n",
        "        f.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "aisin7D-0xRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevance \n",
        "doc_relevance = [{2766: 0, 1574: 1, 2095: 0, 26: 1, 5528: 0}, \n",
        "                 {5094: 1, 3074: 0, 2379: 1, 1833: 1, 5095: 1}, \n",
        "                 {120: 0, 3746: 0, 2555: 1, 373: 0, 3293: 0}, \n",
        "                 {2683: 1, 1749: 1, 3258: 1, 5121: 0, 3663: 0}]\n",
        "\n",
        "createQrel(doc_relevance, query_results, \"CoffeePostsVSM\")"
      ],
      "metadata": {
        "id": "CYBc2zOc0yb-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}